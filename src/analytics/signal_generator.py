from __future__ import annotations

import logging
from typing import Dict, Tuple

import pandas as pd

from src.analytics.features import build_features
from src.analytics.ml import FEATURES, train_ml_model
from src.analytics.scoring import (
    apply_trend_filter,
    corr_penalty,
    liquidity_score,
    momentum_score,
    verdict_from_total,
    vix_score,
)
from src.analytics.statistics import calculate_cot_composite, get_quantile_thresholds
from src.config.settings import get_settings

logger = logging.getLogger(__name__)


def score_asset(asset: str, dfs: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, float, str, float]:
    s = get_settings()
    sc = s.scoring

    # OPTIMIZED: —Å—Ç—Ä–æ–∏–º —Ñ–∏—á–∏ 1 —Ä–∞–∑ (—Å target), –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏ –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤, –∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
    df_all = build_features(dfs, asset, for_signals=False)
    if df_all.empty or len(df_all) < s.signals.min_feature_rows:
        return pd.DataFrame(), 0.0, "No data", 0.0

    df_features = df_all.drop(columns=["target"]) # OPTIMIZED: —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç build_features(..., for_signals=True)

    horizon = s.ml.target_horizon_days
    train_df = df_all
    if len(train_df) > horizon:
        train_df = train_df.iloc[:-horizon]

    latest = df_features.iloc[-1]

    rows = []

    ml_score = 0.0
    if sc.ml_enabled:
        model = train_ml_model(train_df)
        latest_row = latest.reindex(FEATURES).astype(float)
        try:
            # –û—Å—Ç–∞–≤–ª—è–µ–º DataFrame, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ sklearn (–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –æ feature names)
            predicted_return = float(model.predict(pd.DataFrame([latest_row]))[0])
        except Exception:
            predicted_return = 0.0
        ml_score = predicted_return / s.ml.pred_to_score_divisor
        trend_text = "No trend filter"
        if s.scoring.trend_filter_enabled:
            ml_score, trend_text = apply_trend_filter(ml_score, int(latest.get("above_200ma", 1)))
        if predicted_return != 0:
            rows.append(("ML Predicted Return", round(ml_score, 2), f"{predicted_return:.2f}% | {trend_text}"))

    if sc.vix_enabled:
        vix_series = dfs.get("vix", pd.DataFrame()).get("deviation_pct", pd.Series(dtype=float))
        vix_thresh = get_quantile_thresholds(vix_series)
        v_score, v_text = vix_score(float(latest.get("vix_dev", 0.0)), vix_thresh)
        rows.append(("VIX deviation", v_score, v_text))

    if sc.cot_enabled:
        cot_thresh = get_quantile_thresholds(df_features["cot_comm"])
        cot_score, cot_text = calculate_cot_composite(
            float(latest.get("cot_comm", 0.0)),
            float(latest.get("cot_large_inv", 0.0)),
            float(latest.get("z_large", 0.0)),
            cot_thresh,
        )
        rows.append(("COT Composite", cot_score, cot_text))

    if sc.momentum_enabled:
        m_score, m_text = momentum_score(float(latest.get("mom_30d", 0.0)))
        rows.append((f"{asset} 30d momentum", m_score, m_text))

    if sc.liquidity_enabled:
        l_score, l_text = liquidity_score(float(latest.get("dxy_30d", 0.0)), float(latest.get("us10y_30d", 0.0)))
        rows.append(("Liquidity", l_score, l_text))

    if sc.correlation_enabled:
        c_score, c_text = corr_penalty(float(latest.get("spx_corr", 0.0)))
        rows.append(("SPX corr penalty", c_score, c_text))

    df_table = pd.DataFrame(rows, columns=["Factor", "Score", "Rationale"])
    total = float(df_table["Score"].sum())
    verdict = verdict_from_total(total)
    confidence = min(1.0, abs(total) / 5.0)

    return df_table, round(total, 2), verdict, round(confidence, 2)


def generate_conclusion(dfs: Dict[str, pd.DataFrame]):
    per_asset = {}
    for asset in ["BTC", "ETH"]:
        try:
            per_asset[asset] = score_asset(asset, dfs)
        except Exception as e:
            logger.exception("score_asset failed for %s: %s", asset, e)
            per_asset[asset] = (pd.DataFrame(), 0.0, "Neutral", 0.0)

    combined = (per_asset["BTC"][1] + per_asset["ETH"][1]) / 2
    combined_verdict = (
        "üöÄ –°–∏–ª—å–Ω—ã–π –ª–æ–Ω–≥"
        if combined >= 4.0
        else "üìà –õ–æ–Ω–≥"
        if combined >= 2.2
        else "‚öñÔ∏è –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
        if abs(combined) < 1.8
        else "üîª –®–æ—Ä—Ç"
        if combined > -4.0
        else "üõë –°–∏–ª—å–Ω—ã–π —à–æ—Ä—Ç"
    )
    return per_asset, round(combined, 2), combined_verdict


def generate_signals(
    dfs_full: Dict[str, pd.DataFrame],
    asset: str = "BTC",
) -> pd.DataFrame:
    s = get_settings()
    sig = s.signals # OPTIMIZED: –ª–æ–∫–∞–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞, –º–µ–Ω—å—à–µ –∞—Ç—Ä–∏–±—É—Ç–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π
    asset_key = asset.lower()
    df_price = dfs_full.get(asset_key)
    if df_price is None or len(df_price) < sig.min_price_rows:
        return pd.DataFrame(columns=["date", "total_score", "verdict", "signal", "confidence"])

    df_price = df_price.copy()
    df_price["date"] = pd.to_datetime(df_price["date"]).dt.normalize()
    df_price = df_price.sort_values("date").reset_index(drop=True)

    start_i = max(sig.min_start_bars, int(len(df_price) * sig.start_fraction))
    step = int(sig.step_days)
    results = []

    # OPTIMIZED: –∑–∞—Ä–∞–Ω–µ–µ –≥–æ—Ç–æ–≤–∏–º ‚Äú–ø–ª–∞–Ω –Ω–∞—Ä–µ–∑–∫–∏‚Äù –¥–ª—è –∫–∞–∂–¥–æ–≥–æ df:
    # - –µ—Å–ª–∏ date –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç => searchsorted + iloc (O(logN))
    # - –∏–Ω–∞—á–µ fallback –Ω–∞ —Å—Ç–∞—Ä—É—é –º–∞—Å–∫—É (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–ª—è edge-case –Ω–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
    slice_plan = {}
    for k, v in dfs_full.items():
        if v is None or v.empty:
            slice_plan[k] = (None, None, False)
            continue

        # –í–ê–ñ–ù–û: –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞—â–∏—Ç—É –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ "date", —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è (KeyError –æ—Å—Ç–∞—ë—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω—ã–º –∫–∞–∫ —Ä–∞–Ω—å—à–µ).
        date_col = v["date"]
        fast = pd.api.types.is_datetime64_any_dtype(date_col) and getattr(date_col, "is_monotonic_increasing", False)
        date_values = date_col.to_numpy() if fast else None
        slice_plan[k] = (v, date_values, fast)

    # OPTIMIZED: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω dict –ø–æ–¥ sliced, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –∞–ª–ª–æ–∫–∞—Ü–∏–∏ –≤ —Ü–∏–∫–ª–µ
    sliced: Dict[str, pd.DataFrame] = {}

    for i in range(start_i, len(df_price) - step, step):
        current_date = df_price.loc[i, "date"]

        cur64 = current_date.to_datetime64()

        for k, (v, date_values, fast) in slice_plan.items():
            if v is None:
                sliced[k] = pd.DataFrame()
                continue

            if fast:
                # OPTIMIZED: searchsorted + iloc –≤–º–µ—Å—Ç–æ boolean mask (–±—ã—Å—Ç—Ä–µ–µ –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö)
                pos = date_values.searchsorted(cur64, side="right")
                sliced[k] = v.iloc[:pos]
            else:
                # OPTIMIZED: fallback = –∏—Å—Ö–æ–¥–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å—Ç—Ä–æ–∫ –ø—Ä–∏ –Ω–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö df)
                sliced[k] = v[v["date"] <= current_date]

        table, total, verdict, conf = score_asset(asset, sliced)

        vix_df = sliced.get("vix", pd.DataFrame())
        latest_vix = float(vix_df["deviation_pct"].iloc[-1]) if not vix_df.empty and "deviation_pct" in vix_df.columns else 0.0

        # OPTIMIZED: inline dynamic_min_score (—Ç–∞ –∂–µ —Ñ–æ—Ä–º—É–ª–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ –æ–ø–µ—Ä–∞—Ü–∏–π)
        dyn_thr = sig.dyn_min_score_base + sig.dyn_min_score_vix_scale * (latest_vix / sig.dyn_min_score_vix_divisor)

        signal_flag = 1 if total >= dyn_thr else 0

        row = {
            "date": current_date,
            "total_score": total,
            "verdict": verdict,
            "signal": signal_flag,
            "confidence": conf,
            "dyn_min_score": round(dyn_thr, 3),
        }
        if not table.empty:
            # OPTIMIZED: –±–µ–∑ set_index/to_dict (–º–µ–Ω—å—à–µ –∞–ª–ª–æ–∫–∞—Ü–∏–π), –ø–æ—Ä—è–¥–æ–∫ –∫–ª—é—á–µ–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ.
            row.update(dict(zip(table["Factor"].tolist(), table["Score"].tolist())))
        results.append(row)

    if not results:
        return pd.DataFrame(columns=["date", "total_score", "verdict", "signal", "confidence"])

    df_signals = pd.DataFrame(results).sort_values("date").reset_index(drop=True)
    df_signals["date"] = pd.to_datetime(df_signals["date"]).dt.normalize()
    return df_signals